{"componentChunkName":"component---src-templates-documentation-js","path":"/docs/2.6.x/recipes/polyglot/app/","result":{"data":{"pages":{"edges":[{"node":{"id":"e9021eeb-a6bb-59ff-ac01-48c86eabd4a0","fields":{"path":"/docs/2.6.x/installation/","version":"2.6.x","category":"installation"},"frontmatter":{"title":"Installation","description":"How to install Data Flow","path":"installation/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"4d2ef486-b1d3-5021-b5ba-42ec362b1562","fields":{"path":"/docs/2.6.x/installation/local/","version":"2.6.x","category":"installation"},"frontmatter":{"title":"Local Machine","description":"Local Machine Installation Guide","path":"installation/local/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"5677e8b5-0bd6-5e56-86b2-885107a13e1b","fields":{"path":"/docs/2.6.x/installation/local/docker/","version":"2.6.x","category":"installation"},"frontmatter":{"title":"Docker Compose","description":"Installation using Docker Compose","path":"installation/local/docker","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"f8b9d584-3559-5744-beba-9e5d2ca30de6","fields":{"path":"/docs/2.6.x/installation/local/docker-customize/","version":"2.6.x","category":"installation"},"frontmatter":{"title":"Docker Compose Customization","description":"Customize the Docker Compose installation","path":"installation/local/docker-customize","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"1e019f6e-b12c-5dfb-af9f-0ee64023c52c","fields":{"path":"/docs/2.6.x/installation/local/manual/","version":"2.6.x","category":"installation"},"frontmatter":{"title":"Manual","description":"Manual installation","path":"installation/local/manual","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"90cc84da-bf5e-51a4-8bf1-27e4f643ba6e","fields":{"path":"/docs/2.6.x/installation/cloudfoundry/","version":"2.6.x","category":"installation"},"frontmatter":{"title":"Cloud Foundry","description":"Data Flow Cloud Foundry Installation Guide","path":"installation/cloudfoundry/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"dcde0625-3190-504f-a1a6-f56e72fdc7ae","fields":{"path":"/docs/2.6.x/installation/cloudfoundry/cf-cli/","version":"2.6.x","category":"installation"},"frontmatter":{"title":"Cloud Foundry CLI","description":"Install using the Cloud Foundry CLI","path":"installation/cloudfoundry/cf-cli","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"a5ffaf9e-68c1-57e6-82d6-6164f5f93c0b","fields":{"path":"/docs/2.6.x/installation/cloudfoundry/cf-local/","version":"2.6.x","category":"installation"},"frontmatter":{"title":"Running locally","description":"Configure the local servers to deploy to Cloud Foundry","path":"installation/cloudfoundry/cf-local","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"4477369a-b2fe-5745-b092-e3b67713d013","fields":{"path":"/docs/2.6.x/installation/kubernetes/","version":"2.6.x","category":"installation"},"frontmatter":{"title":"Kubernetes","description":"Data Flow Kubernetes Installation Guide","path":"installation/kubernetes/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"119f4320-d2f6-5cbf-a9a9-8284623dd171","fields":{"path":"/docs/2.6.x/installation/kubernetes/creating-a-cluster/","version":"2.6.x","category":"installation"},"frontmatter":{"title":"Creating a Cluster","description":"Creating a Kubernetes Cluster","path":"installation/kubernetes/creating-a-cluster","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"db871ceb-3a12-5869-ae62-719e896b1db0","fields":{"path":"/docs/2.6.x/installation/kubernetes/helm/","version":"2.6.x","category":"installation"},"frontmatter":{"title":"Helm","description":"Installation using Helm","path":"installation/kubernetes/helm","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"caf737ab-3988-5332-8aff-f4623ff1bd1f","fields":{"path":"/docs/2.6.x/installation/kubernetes/kubectl/","version":"2.6.x","category":"installation"},"frontmatter":{"title":"kubectl","description":"Installation using kubectl","path":"installation/kubernetes/kubectl","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"c5fa77bd-d62a-5098-bcca-70542ab0c26e","fields":{"path":"/docs/2.6.x/installation/kubernetes/compatibility/","version":"2.6.x","category":"installation"},"frontmatter":{"title":"Kubernetes Compatibility","description":"Compatibility with Kubernetes Versions","path":"installation/kubernetes/compatibility","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"929fcfcf-1990-5367-af35-a9e781a72e6b","fields":{"path":"/docs/2.6.x/concepts/","version":"2.6.x","category":"concepts"},"frontmatter":{"title":"Concepts","description":"Core Concepts in Spring Cloud Data Flow","path":"concepts/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"99224add-aea3-5da9-b131-c7f449c8d48e","fields":{"path":"/docs/2.6.x/concepts/architecture/","version":"2.6.x","category":"concepts"},"frontmatter":{"title":"Architecture","description":"Introduction to Data Flow's Architecture.","path":"concepts/architecture/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"d61c129f-87f4-5650-96ef-5c4d102df471","fields":{"path":"/docs/2.6.x/concepts/streams/","version":"2.6.x","category":"concepts"},"frontmatter":{"title":"Stream Processing","description":"Stream Processing Framework and Concepts","path":"concepts/streams/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"89ecf7e1-75c6-5d48-a6cc-ce96d27828d2","fields":{"path":"/docs/2.6.x/concepts/batch-jobs/","version":"2.6.x","category":"concepts"},"frontmatter":{"title":"Batch Processing","description":"Batch Processing Framework and Concepts","path":"concepts/batch-jobs/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"3c0c3e30-fbef-5bc9-a751-9a98ce2623c3","fields":{"path":"/docs/2.6.x/concepts/monitoring/","version":"2.6.x","category":"concepts"},"frontmatter":{"title":"Monitoring","description":"Runtime monitoring of Stream data pipelines","path":"concepts/monitoring/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"f42c5920-63af-5427-8ec4-da4cec31d9d3","fields":{"path":"/docs/2.6.x/concepts/tooling/","version":"2.6.x","category":"concepts"},"frontmatter":{"title":"Tooling","description":"Dashboard and Shell","path":"concepts/tooling/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"de52e1a6-ded9-53a7-9880-1912d060def0","fields":{"path":"/docs/2.6.x/stream-developer-guides/","version":"2.6.x","category":"stream-developer-guides"},"frontmatter":{"title":"Stream Developer guides ","description":"Learn how to create Streaming data pipelines using prebuilt microservices or create your own.","path":"stream-developer-guides/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"25aa7b2c-d0a7-594b-b2e8-5688e30d9b2e","fields":{"path":"/docs/2.6.x/stream-developer-guides/getting-started/","version":"2.6.x","category":"stream-developer-guides"},"frontmatter":{"title":"Getting Started","description":"Getting Started with Stream Processing","path":"stream-developer-guides/getting-started/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"662bc53d-bfe5-573e-aff6-90c1ac01c219","fields":{"path":"/docs/2.6.x/stream-developer-guides/getting-started/stream/","version":"2.6.x","category":"stream-developer-guides"},"frontmatter":{"title":"Stream Processing","description":"Create and deploy a streaming data pipeline using prebuilt applications on your Local Machine","path":"stream-developer-guides/getting-started/stream/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"5e286ba1-bf9e-5bef-b317-439cee0dad6a","fields":{"path":"/docs/2.6.x/stream-developer-guides/streams/","version":"2.6.x","category":"stream-developer-guides"},"frontmatter":{"title":"Stream Development","description":"Stream Processing Developer Guide","path":"stream-developer-guides/streams/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"330b159e-9210-5f4d-9df0-7975a8b80df3","fields":{"path":"/docs/2.6.x/stream-developer-guides/streams/standalone-stream-rabbitmq/","version":"2.6.x","category":"stream-developer-guides"},"frontmatter":{"title":"Stream Application Development on RabbitMQ","description":"Create your own microservices for Stream processing using RabbitMQ and deploy them manually","path":"stream-developer-guides/streams/standalone-stream-rabbitmq/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"f4886ff1-ae23-51fe-a76c-c5d8a688dd5b","fields":{"path":"/docs/2.6.x/stream-developer-guides/streams/standalone-stream-kafka/","version":"2.6.x","category":"stream-developer-guides"},"frontmatter":{"title":"Stream Application Development on Apache Kafka","description":"Create your own microservices for Stream processing using Apache Kafka and deploy them manually","path":"stream-developer-guides/streams/standalone-stream-kafka/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"cabf8341-e0bf-5bb2-afa0-4d99e24ce1e8","fields":{"path":"/docs/2.6.x/stream-developer-guides/streams/data-flow-stream/","version":"2.6.x","category":"stream-developer-guides"},"frontmatter":{"title":"Stream Processing using Spring Cloud Data Flow","description":"Create and Deploy a Stream Processing Pipeline using Spring Cloud Data Flow","path":"stream-developer-guides/streams/data-flow-stream/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"e5aceaf2-0ade-5d25-8c07-46dc78c41596","fields":{"path":"/docs/2.6.x/stream-developer-guides/streams/stream-other-binders/","version":"2.6.x","category":"stream-developer-guides"},"frontmatter":{"title":"Spring Application Development on other Messaging Middleware","description":"Create your own microservices for Stream processing using other messaging middleware such as Google Pub/Sub, Amazon Kinesis, and Solace JMS","path":"stream-developer-guides/streams/stream-other-binders/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"5cf102aa-7ea7-56e2-9efd-6d9ecca536fe","fields":{"path":"/docs/2.6.x/stream-developer-guides/programming-models/","version":"2.6.x","category":"stream-developer-guides"},"frontmatter":{"title":"Programming Models","description":"Programming models","path":"stream-developer-guides/programming-models/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"cb665729-22e1-5fc3-9022-c0dd45f194cb","fields":{"path":"/docs/2.6.x/stream-developer-guides/continuous-delivery/","version":"2.6.x","category":"stream-developer-guides"},"frontmatter":{"title":"Continuous Delivery","description":"CD using Skipper","path":"stream-developer-guides/continuous-delivery/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"a11a49c7-beb0-591c-9eb3-edb64c83c6b1","fields":{"path":"/docs/2.6.x/stream-developer-guides/continuous-delivery/cd-basics/","version":"2.6.x","category":"stream-developer-guides"},"frontmatter":{"title":"Continuous Delivery of streaming applications","description":"Continuous Delivery of Streaming applications","path":"stream-developer-guides/continuous-delivery/cd-basics/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"26860f37-faed-5b74-b92b-26fb867505ca","fields":{"path":"/docs/2.6.x/stream-developer-guides/troubleshooting/","version":"2.6.x","category":"stream-developer-guides"},"frontmatter":{"title":"Troubleshooting","description":"Troubleshooting Streams","path":"stream-developer-guides/troubleshooting/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"48779233-7811-551b-bb00-1bc1a9729927","fields":{"path":"/docs/2.6.x/stream-developer-guides/troubleshooting/debugging-stream-apps/","version":"2.6.x","category":"stream-developer-guides"},"frontmatter":{"title":"Debugging Stream Applications","description":"Debugging Stream Applications outside of Data Flow","path":"stream-developer-guides/troubleshooting/debugging-stream-apps/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"5f57c55d-32e4-59fc-91c1-8a52bcbb48ee","fields":{"path":"/docs/2.6.x/stream-developer-guides/troubleshooting/debugging-scdf-streams/","version":"2.6.x","category":"stream-developer-guides"},"frontmatter":{"title":"Debugging Stream applications deployed by Data Flow","description":"Debugging Data Flow Stream deployments","path":"stream-developer-guides/troubleshooting/debugging-scdf-streams/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"37a22652-17b0-5d93-8ca8-bca78334ea76","fields":{"path":"/docs/2.6.x/batch-developer-guides/","version":"2.6.x","category":"batch-developer-guides"},"frontmatter":{"title":"Batch Developer guides ","description":"Learn how to create Batch data pipelines using prebuilt microservices or create your own","path":"batch-developer-guides/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"801a3829-0592-5238-a423-a7401fda6d75","fields":{"path":"/docs/2.6.x/batch-developer-guides/getting-started/","version":"2.6.x","category":"batch-developer-guides"},"frontmatter":{"title":"Getting Started","description":"Getting Started with Batch","path":"batch-developer-guides/getting-started/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"6d3eaaf0-06a0-5b15-9e34-22e5a3bc3c4a","fields":{"path":"/docs/2.6.x/batch-developer-guides/getting-started/task/","version":"2.6.x","category":"batch-developer-guides"},"frontmatter":{"title":"Task Processing","description":"Create and deploy a simple Task pipeline using a prebuilt Task application on your local machine","path":"batch-developer-guides/getting-started/task/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"1c467d57-4330-57b5-a082-ce1d66c7b97d","fields":{"path":"/docs/2.6.x/batch-developer-guides/batch/","version":"2.6.x","category":"batch-developer-guides"},"frontmatter":{"title":"Batch Development","description":"Batch Developer Guide","path":"batch-developer-guides/batch/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"0bc43ff5-11dc-538a-8a22-dccbbb01b166","fields":{"path":"/docs/2.6.x/batch-developer-guides/batch/spring-task/","version":"2.6.x","category":"batch-developer-guides"},"frontmatter":{"title":"Simple Task","description":"Create a simple Spring Boot Application using Spring Cloud Task","path":"batch-developer-guides/batch/spring-task/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"07cab399-4800-5892-b85c-2a7458c18d5f","fields":{"path":"/docs/2.6.x/batch-developer-guides/batch/spring-batch/","version":"2.6.x","category":"batch-developer-guides"},"frontmatter":{"title":"Spring Batch Jobs","description":"Create a Spring Batch Job","path":"batch-developer-guides/batch/spring-batch/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"c82bad2b-2e5d-5771-bce6-27066ff3ef82","fields":{"path":"/docs/2.6.x/batch-developer-guides/batch/data-flow-simple-task/","version":"2.6.x","category":"batch-developer-guides"},"frontmatter":{"title":"Register and Launch a Spring Cloud Task application using Data Flow","description":"Register and Launch a Spring Cloud Task application using Data Flow","path":"batch-developer-guides/batch/data-flow-simple-task/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"cec2f163-1cd9-5052-981e-89a5e085cb84","fields":{"path":"/docs/2.6.x/batch-developer-guides/batch/data-flow-spring-batch/","version":"2.6.x","category":"batch-developer-guides"},"frontmatter":{"title":"Register and launch a Spring Batch application using Data Flow","description":"Register and launch a Spring Batch application using Data Flow","path":"batch-developer-guides/batch/data-flow-spring-batch/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"d100d3c9-ac7e-578d-9707-d7af65827827","fields":{"path":"/docs/2.6.x/batch-developer-guides/batch/data-flow-composed-task/","version":"2.6.x","category":"batch-developer-guides"},"frontmatter":{"title":"Create and launch a Composed Task using Data Flow","description":"Create and launch a Composed Task using Data Flow","path":"batch-developer-guides/batch/data-flow-composed-task/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"c1352ada-7f61-57d2-976d-3f49bd2b3ab6","fields":{"path":"/docs/2.6.x/batch-developer-guides/batch/data-flow-simple-task-kubernetes/","version":"2.6.x","category":"batch-developer-guides"},"frontmatter":{"title":"Deploying a task application on Kubernetes with Data Flow","description":"Guide to deploying spring-cloud-stream-task applications on Kubernetes using Spring Cloud Data Flow","path":"batch-developer-guides/batch/data-flow-simple-task-kubernetes/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"d903c65f-f587-5023-ad49-d9a2da203603","fields":{"path":"/docs/2.6.x/batch-developer-guides/batch/data-flow-simple-task-cloudfoundry/","version":"2.6.x","category":"batch-developer-guides"},"frontmatter":{"title":"Deploying a task application on Cloud Foundry using Spring Cloud Data Flow","description":"Guide to deploying spring-cloud-stream-task applications on Cloud Foundry using Spring Cloud Data Flow","path":"batch-developer-guides/batch/data-flow-simple-task-cloudfoundry/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"db6a5270-4b1c-550c-ab60-345a6cb8e8bd","fields":{"path":"/docs/2.6.x/batch-developer-guides/continuous-deployment/","version":"2.6.x","category":"batch-developer-guides"},"frontmatter":{"title":"Continuous Deployment","description":"Continuous Deployment for task applications","path":"batch-developer-guides/continuous-deployment/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"27640e7d-20c3-5857-a9f7-e5e74e56a98d","fields":{"path":"/docs/2.6.x/batch-developer-guides/continuous-deployment/cd-basics/","version":"2.6.x","category":"batch-developer-guides"},"frontmatter":{"title":"Continuous Deployment of task applications","description":"This section discusses how to use Continuous Deployment of Tasks in SCDF","path":"batch-developer-guides/continuous-deployment/cd-basics/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"1044acb3-91f1-5a91-9187-d6e5d33133e4","fields":{"path":"/docs/2.6.x/batch-developer-guides/troubleshooting/","version":"2.6.x","category":"batch-developer-guides"},"frontmatter":{"title":"Troubleshooting","description":"Troubleshooting Batch Jobs","path":"batch-developer-guides/troubleshooting/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"460d5979-e8e4-5a5c-bbf6-db7ee54a471f","fields":{"path":"/docs/2.6.x/batch-developer-guides/troubleshooting/debugging-task-apps/","version":"2.6.x","category":"batch-developer-guides"},"frontmatter":{"title":"Debugging Batch applications","description":"Debugging Batch applications","path":"batch-developer-guides/troubleshooting/debugging-task-apps/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"b0663c20-e821-5837-a799-a53686f5c67b","fields":{"path":"/docs/2.6.x/batch-developer-guides/troubleshooting/debugging-scdf-tasks/","version":"2.6.x","category":"batch-developer-guides"},"frontmatter":{"title":"Debugging Batch applications deployed by Data Flow","description":"Debugging Batch applications deployed by Data Flow","path":"batch-developer-guides/troubleshooting/debugging-scdf-tasks/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"0db0658d-b083-5c3d-b43c-f33701394dbc","fields":{"path":"/docs/2.6.x/feature-guides/","version":"2.6.x","category":"feature-guides"},"frontmatter":{"title":"Feature guides","description":"High level overview of Data Flow Features","path":"feature-guides/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"3d4a79f7-bc01-57f4-8fee-1ec7f8038396","fields":{"path":"/docs/2.6.x/feature-guides/general/","version":"2.6.x","category":"feature-guides"},"frontmatter":{"title":"General","description":"General Features in Data Flow","path":"feature-guides/general/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"72147749-774f-5535-83d3-d0237da98171","fields":{"path":"/docs/2.6.x/feature-guides/general/application-metadata/","version":"2.6.x","category":"feature-guides"},"frontmatter":{"title":"Application Metadata","description":"Create and use application properties metadata","path":"feature-guides/general/application-metadata/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"bdb03de9-49e7-54cc-9c78-012a7d62f551","fields":{"path":"/docs/2.6.x/feature-guides/streams/","version":"2.6.x","category":"feature-guides"},"frontmatter":{"title":"Streams","description":"Stream Features in Data Flow","path":"feature-guides/streams/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"9e46d55d-12bb-5dfb-8ae8-1eb0cd5af0d4","fields":{"path":"/docs/2.6.x/feature-guides/streams/deployment-properties/","version":"2.6.x","category":"feature-guides"},"frontmatter":{"title":"Deployment Properties","description":"Initiate a stream deployment with deployment property overrides","path":"feature-guides/streams/deployment-properties/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"9cfa8e02-c993-57f5-9b80-79ac6b30bacd","fields":{"path":"/docs/2.6.x/feature-guides/streams/function-composition/","version":"2.6.x","category":"feature-guides"},"frontmatter":{"title":"Composing Functions","description":"Daisy-chain Java functions in an existing Spring Cloud Stream application","path":"feature-guides/streams/function-composition/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"4bf293a4-98e7-5e83-ba2f-d691ea4ea8e7","fields":{"path":"/docs/2.6.x/feature-guides/streams/named-destinations/","version":"2.6.x","category":"feature-guides"},"frontmatter":{"title":"Named Destinations","description":"Use the Named Destinations to interact with the Topics/Queues directly","path":"feature-guides/streams/named-destinations/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"f2492bb6-cf38-56b7-a5b0-7b480d93831b","fields":{"path":"/docs/2.6.x/feature-guides/streams/monitoring/","version":"2.6.x","category":"feature-guides"},"frontmatter":{"title":"Stream Monitoring","description":"Monitoring streaming data pipelines with Prometheus and InfluxDB","path":"feature-guides/streams/monitoring/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"ea8d17bc-832b-5010-8493-0057ce1e3c9e","fields":{"path":"/docs/2.6.x/feature-guides/streams/stream-application-dsl/","version":"2.6.x","category":"feature-guides"},"frontmatter":{"title":"Stream Application DSL","description":"Learn how to use the Stream Application DSL","path":"feature-guides/streams/stream-application-dsl/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"7e0c92af-5ac0-525e-886d-24ef860121cc","fields":{"path":"/docs/2.6.x/feature-guides/streams/labels/","version":"2.6.x","category":"feature-guides"},"frontmatter":{"title":"Labeling Applications","description":"Label the stream applications to uniquely interact with them","path":"feature-guides/streams/labels/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"c434626b-145c-511f-8591-9c3ddcb282a3","fields":{"path":"/docs/2.6.x/feature-guides/streams/application-count/","version":"2.6.x","category":"feature-guides"},"frontmatter":{"title":"Application Count","description":"Initiate stream deployment with multiple application instances","path":"feature-guides/streams/application-count/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"1f734409-9f01-5699-8906-d4b81b4fc7df","fields":{"path":"/docs/2.6.x/feature-guides/streams/fanin-fanout/","version":"2.6.x","category":"feature-guides"},"frontmatter":{"title":"Fan-in and Fan-out","description":"Publish and subscribe to multiple destinations using the fan-in and fan-out capabilities","path":"feature-guides/streams/fanin-fanout/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"8a34cc56-301c-5298-8be5-5aa9a712d156","fields":{"path":"/docs/2.6.x/feature-guides/streams/partitioning/","version":"2.6.x","category":"feature-guides"},"frontmatter":{"title":"Data Partitioning","description":"Learn more about data partitioning support to build stateful streaming data pipelines","path":"feature-guides/streams/partitioning/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"3e10f1c9-14b7-50e3-b295-7fac6fd8de79","fields":{"path":"/docs/2.6.x/feature-guides/streams/scaling/","version":"2.6.x","category":"feature-guides"},"frontmatter":{"title":"Scaling","description":"Scaling streaming data pipeline with Spring Cloud Data Flow","path":"feature-guides/streams/scaling/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"5c27a2e4-32bb-5f8c-8a71-120c286b8c70","fields":{"path":"/docs/2.6.x/feature-guides/streams/java-dsl/","version":"2.6.x","category":"feature-guides"},"frontmatter":{"title":"Java DSL","description":"Programmatically create streams using the Java DSL","path":"feature-guides/streams/java-dsl/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"531c7f03-b972-5bb7-a252-a7a584fc8ef5","fields":{"path":"/docs/2.6.x/feature-guides/streams/taps/","version":"2.6.x","category":"feature-guides"},"frontmatter":{"title":"Tapping a Stream","description":"Create a stream from another stream without interrupting the data processing","path":"feature-guides/streams/taps/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"76e86fcf-b096-55af-b668-54a9ee785297","fields":{"path":"/docs/2.6.x/feature-guides/batch/","version":"2.6.x","category":"feature-guides"},"frontmatter":{"title":"Batch","description":"Batch Features in Data Flow","path":"feature-guides/batch/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"dd3f1828-4361-5163-ac19-dfc79b6feeff","fields":{"path":"/docs/2.6.x/feature-guides/batch/deployment-properties/","version":"2.6.x","category":"feature-guides"},"frontmatter":{"title":"Deployment Properties","description":"Initiate a Batch deployment with deployment property overrides","path":"feature-guides/batch/deployment-properties/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"db544e92-6c62-5c3e-b630-b1a8a0a65300","fields":{"path":"/docs/2.6.x/feature-guides/batch/scheduling/","version":"2.6.x","category":"feature-guides"},"frontmatter":{"title":"Scheduling Batch Jobs","description":"Learn how to schedule Batch Jobs","path":"feature-guides/batch/scheduling/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"939b3288-80cc-5c8c-b608-6d3e6a3566fa","fields":{"path":"/docs/2.6.x/feature-guides/batch/partitioning/","version":"2.6.x","category":"feature-guides"},"frontmatter":{"title":"Remote Partitioned Batch Job","description":"Learn more about partitioning support for Batch Jobs","path":"feature-guides/batch/partitioning/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"3b6f6285-0025-58ac-a678-507000366895","fields":{"path":"/docs/2.6.x/feature-guides/batch/monitoring/","version":"2.6.x","category":"feature-guides"},"frontmatter":{"title":"Task Monitoring","description":"Monitoring task data pipelines with InfluxDB","path":"feature-guides/batch/monitoring/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"88baffea-f888-5470-990b-3ae96079720a","fields":{"path":"/docs/2.6.x/feature-guides/batch/restarting/","version":"2.6.x","category":"feature-guides"},"frontmatter":{"title":"Restarting Batch Jobs","description":"Learn how to restart Batch Jobs","path":"feature-guides/batch/restarting/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"5ccd8a85-54c4-5734-960d-61cd737d976c","fields":{"path":"/docs/2.6.x/feature-guides/batch/composed-task/","version":"2.6.x","category":"feature-guides"},"frontmatter":{"title":"Composed Tasks","description":"Learn how to create and manage composed tasks","path":"feature-guides/batch/composed-task/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"cf488e7a-a79a-58a1-84c7-f239b00f0a36","fields":{"path":"/docs/2.6.x/recipes/","version":"2.6.x","category":"recipes"},"frontmatter":{"title":"Recipes","description":"Recipes that help solve some common use-cases","path":"recipes/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"832b2669-96c0-5acf-955f-a8092425bd75","fields":{"path":"/docs/2.6.x/recipes/polyglot/","version":"2.6.x","category":"recipes"},"frontmatter":{"title":"Polyglot","description":"Using multiple programming languages","path":"recipes/polyglot/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"4c7ade10-63e9-5c59-b33c-dc10405cb8bc","fields":{"path":"/docs/2.6.x/recipes/polyglot/processor/","version":"2.6.x","category":"recipes"},"frontmatter":{"title":"Python Stream Processor","description":"Python Application as a Data Flow Stream Processor","path":"recipes/polyglot/processor/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"8b17d791-bab2-56cc-adfc-cf2ce3ebc3c8","fields":{"path":"/docs/2.6.x/recipes/polyglot/task/","version":"2.6.x","category":"recipes"},"frontmatter":{"title":"Python Task","description":"Create and Deploy a Python Task","path":"recipes/polyglot/task/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"e6de6439-cf64-56a5-aad3-35e69c5f0ede","fields":{"path":"/docs/2.6.x/recipes/polyglot/app/","version":"2.6.x","category":"recipes"},"frontmatter":{"title":"Python Application","description":"Create and Deploy a Python Application in a Stream","path":"recipes/polyglot/app/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"f0245349-2811-5319-a15e-049eb69ead0a","fields":{"path":"/docs/2.6.x/recipes/rabbitmq/","version":"2.6.x","category":"recipes"},"frontmatter":{"title":"RabbitMQ","description":"RabbitMQ","path":"recipes/rabbitmq/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"040ded45-231f-5c98-a3ee-b79f3e463adf","fields":{"path":"/docs/2.6.x/recipes/rabbitmq/rabbit-source-sink/","version":"2.6.x","category":"recipes"},"frontmatter":{"title":"RabbitMQ as Source and Sink","description":"RabbitMQ as Source and Sink + RabbitMQ binder","path":"recipes/rabbitmq/rabbit-source-sink/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"58377b55-b637-5353-aad8-ea5634b5ba70","fields":{"path":"/docs/2.6.x/recipes/kafka/","version":"2.6.x","category":"recipes"},"frontmatter":{"title":"Apache Kafka","description":"Kafka","path":"recipes/kafka/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"96d36967-fb2f-5188-99c1-1ab07a5669dd","fields":{"path":"/docs/2.6.x/recipes/kafka/ext-kafka-cluster-cf/","version":"2.6.x","category":"recipes"},"frontmatter":{"title":"External Kafka Cluster","description":"Connect to an external Kafka Cluster from Cloud Foundry","path":"recipes/kafka/ext-kafka-cluster-cf/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"f8d244b8-35e6-5dd1-8c0e-a0f2683cc8f1","fields":{"path":"/docs/2.6.x/recipes/kinesis/","version":"2.6.x","category":"recipes"},"frontmatter":{"title":"Amazon Kinesis","description":"Amazon Kinesis","path":"recipes/kinesis/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"4dd53e72-e7ae-5ff7-a3c0-f45db20dffdd","fields":{"path":"/docs/2.6.x/recipes/kinesis/simple-producer-consumer/","version":"2.6.x","category":"recipes"},"frontmatter":{"title":"Amazon Kinesis Binder","description":"A sample of Spring Cloud Stream + Amazon Kinesis Binder in action","path":"recipes/kinesis/simple-producer-consumer/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"44ad6f69-e0bf-5a2c-b1e0-6e44c4f32960","fields":{"path":"/docs/2.6.x/recipes/multi-platform-deployment/","version":"2.6.x","category":"recipes"},"frontmatter":{"title":"Multiple Platform Deployments","description":"Multiple Platform Deployments","path":"recipes/multi-platform-deployment/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"e151daf9-0907-5888-a264-fd1225e70752","fields":{"path":"/docs/2.6.x/recipes/multi-platform-deployment/multiple-platform-accounts/","version":"2.6.x","category":"recipes"},"frontmatter":{"title":"Role of Multiple Platform Deployments","description":"A walk-through of multiple platform requirements and the configurations for Cloud Foundry and Kubernetes","path":"recipes/multi-platform-deployment/multiple-platform-accounts","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"7d0b675d-d227-5eae-8e6a-bcdfe22e24a1","fields":{"path":"/docs/2.6.x/recipes/multi-platform-deployment/multi-platform-task/","version":"2.6.x","category":"recipes"},"frontmatter":{"title":"Multiple Platform support for Tasks","description":"Learn how to launch and schedule tasks across multiple platforms","path":"recipes/multi-platform-deployment/multi-platform-task","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"e2bb5e49-03c6-572d-909f-ba0175bac7a1","fields":{"path":"/docs/2.6.x/recipes/scaling/","version":"2.6.x","category":"recipes"},"frontmatter":{"title":"Scaling","description":"Prometheus and Data Flow to autoscale streaming data pipelines","path":"recipes/scaling/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"ebe9d02c-933f-575a-b6a2-70902f36adbb","fields":{"path":"/docs/2.6.x/recipes/scaling/manual-scaling/","version":"2.6.x","category":"recipes"},"frontmatter":{"title":"Manual Scaling","description":"Scale applications using SCDF Shell","path":"recipes/scaling/manual-scaling/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"1a8da8d0-dc19-588e-842b-6753e5f680a3","fields":{"path":"/docs/2.6.x/recipes/scaling/autoscaling/","version":"2.6.x","category":"recipes"},"frontmatter":{"title":"Autoscaling","description":"Autoscale streaming data pipeline with SCDF and Prometheus","path":"recipes/scaling/autoscaling/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"41246959-ff77-5531-8710-baa307c71cf3","fields":{"path":"/docs/2.6.x/recipes/batch/","version":"2.6.x","category":"recipes"},"frontmatter":{"title":"Batch","description":"Using Spring Cloud Data Flow with Spring Batch","path":"recipes/batch/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"376ac9e8-46be-5ea1-8ab7-a651e3d9acc4","fields":{"path":"/docs/2.6.x/recipes/batch/batch-only-mode/","version":"2.6.x","category":"recipes"},"frontmatter":{"title":"Batch-only Mode","description":"Set up Spring Cloud Data Flow to use only batch and not streams","path":"recipes/batch/batch-only-mode/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"a26aab98-8b26-5318-8b76-131c7c6c9fd6","fields":{"path":"/docs/2.6.x/recipes/batch/sftp-to-jdbc/","version":"2.6.x","category":"recipes"},"frontmatter":{"title":"SFTP to JDBC","description":"Ingest Files from SFTP to a JDBC data store using Data Flow and Spring Batch","path":"recipes/batch/sftp-to-jdbc/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"51203ff4-d111-57ea-bf53-15b3f24e38d0","fields":{"path":"/docs/2.6.x/recipes/functional-apps/","version":"2.6.x","category":"recipes"},"frontmatter":{"title":"Functional Applications","description":"Using Functional Approach in Spring Cloud Stream applications","path":"recipes/functional-apps/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"b86a08d0-bac2-5780-b8e1-eb48f0a4f9da","fields":{"path":"/docs/2.6.x/recipes/functional-apps/scst-function-bindings/","version":"2.6.x","category":"recipes"},"frontmatter":{"title":"Functional Applications","description":"Configuring the Spring Cloud Stream Functional applications","path":"recipes/functional-apps/scst-function-bindings/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"0d4fc819-caeb-567e-98c1-703594e0a8f5","fields":{"path":"/docs/2.6.x/recipes/cloud-providers/","version":"2.6.x","category":"recipes"},"frontmatter":{"title":"Cloud Providers","description":"Using functionality provided by cloud providers","path":"recipes/cloud-providers/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"6c4c5dda-3035-5a42-86a6-4f69957a5031","fields":{"path":"/docs/2.6.x/recipes/cloud-providers/gke-regional-clusters/","version":"2.6.x","category":"recipes"},"frontmatter":{"title":"GKE Regional Clusters","description":"Deploying Spring Cloud Data Flow to a GKE Regional Cluster","path":"recipes/cloud-providers/gke-regional-clusters/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"861688d5-2c38-508d-978e-640323c6d6f8","fields":{"path":"/docs/2.6.x/resources/","version":"2.6.x","category":"resources"},"frontmatter":{"title":"Resources","description":"Sample Applications, References Docs, Videos, Blogs...","path":"resources/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"ae36aa66-bfc0-5e25-a8d5-fadf9923ab74","fields":{"path":"/docs/2.6.x/resources/reference-docs/","version":"2.6.x","category":"resources"},"frontmatter":{"title":"Reference Documentation","description":"Collection of reference guides for Spring Cloud Data Flow","path":"resources/reference-docs/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"4c5f2c8b-1d31-5ae4-a8b6-72ce57ccbfb0","fields":{"path":"/docs/2.6.x/resources/samples/","version":"2.6.x","category":"resources"},"frontmatter":{"title":"Samples","description":"Collection of samples","path":"resources/samples/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"6338daa2-c819-5f7d-80c8-544b0f84079d","fields":{"path":"/docs/2.6.x/resources/faq/","version":"2.6.x","category":"resources"},"frontmatter":{"title":"Frequently Asked Questions","description":"","path":"resources/faq/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"9ceb4a4c-e3d7-51ce-ad2b-92062de9b87b","fields":{"path":"/docs/2.6.x/applications/","version":"2.6.x","category":"applications"},"frontmatter":{"title":"Applications","description":"Using stream and task applications","path":"applications/","meta_title":null,"meta_description":null,"keywords":null}}},{"node":{"id":"ad96a136-c832-5548-ad1d-fa05028763c0","fields":{"path":"/docs/2.6.x/applications/pre-packaged/","version":"2.6.x","category":"applications"},"frontmatter":{"title":"Pre-packaged Applications","description":"Pre-packaged stream and task applications","path":"applications/pre-packaged","meta_title":null,"meta_description":null,"keywords":["application","pre-packaged"]}}}]},"page":{"html":"<h1 id=\"create-and-deploy-a-python-application\" style=\"position:relative;\"><a href=\"#create-and-deploy-a-python-application\" aria-label=\"create and deploy a python application permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Create and Deploy a Python Application</h1>\n<p>This recipe illustrates how to deploy a Python script as an Data Flow <a href=\"https://docs.spring.io/spring-cloud-dataflow/docs/2.6.3/reference/htmlsingle/#spring-cloud-dataflow-stream-app-dsl\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">application</a>.\nUnlike the other applications types (e.g. <code class=\"language-text\">source</code>, <code class=\"language-text\">processor</code> or <code class=\"language-text\">sink</code>), Data Flow does not set deployment properties that wire up producers and consumers when deploying the <code class=\"language-text\">app</code> application type.\nIt is the developerâ€™s responsibility to 'wire up' the multiple applications when deploying in order for them to communicate by using deployment properties.</p>\n<p>The recipe creates a data processing pipelines that dispatches <code class=\"language-text\">input</code> stream of timestamps to either <code class=\"language-text\">even</code> or <code class=\"language-text\">odd</code> downstream channels.\nTechnically the recipe implements the <a href=\"https://www.enterpriseintegrationpatterns.com/patterns/messaging/DynamicRouter.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Dynamic Router</a> integration pattern.\nThe Pipeline takes time's source <code class=\"language-text\">timestamps</code> messages from an <code class=\"language-text\">timeDest</code> input channel, depending on the timestamp value it routes the message to dedicated <code class=\"language-text\">evenDest</code> or <code class=\"language-text\">oddDest</code> downstream channels.</p>\n<p>The following diagram shows the architecture of the cafe processing pipelines.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 695px;\"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 37%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsSAAALEgHS3X78AAABT0lEQVQoz12RPVLDMBCFfTUOQs8tcgc6rgAlFQ1DZmjoaIAmySSRbCu2Ff/KsmXLj5USMjGa0c+spLfv2w2macJl0Hm6mhTwu53c6TTGcUSnO5i+x2At0rpARrPutL8P3GLpk7EjrqTP+n+RCYPR6LsOUkqs+B5JmiI+ZtiyLdabFaJjinGyCFyWOJcQeQbZVF6kVQq6bb1UWDR4+dxg+bWDIleW3ldd6/esKkhw5wVj+u+MBVLV4BGDCDl4KlDWNVgYYh2FhNXi6f0bN4tn3N6/QpTNjMAJZKpC0pRoTX9CboidHSKIJAaXCUbK7LK3g/EPlj8Mdw9vWDx+oKDaDcYgFgKKKLpxIOwUQqbIydilhg4hVw20Mfg/XF2Uuy8LaK1RlCX22QG1IyEixnfYrlcIyYzrQzDr8rkR805fxc/uHYU3ohU40bGIIylzH/sFUKoYcqP66LsAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n        <source\n          srcset=\"/static/16c1db1f2ea6b920668c0c2e3c7eb04f/507e6/polyglot-python-app-architecture.webp 200w,\n/static/16c1db1f2ea6b920668c0c2e3c7eb04f/28a80/polyglot-python-app-architecture.webp 400w,\n/static/16c1db1f2ea6b920668c0c2e3c7eb04f/be6ed/polyglot-python-app-architecture.webp 695w\"\n          sizes=\"(max-width: 695px) 100vw, 695px\"\n          type=\"image/webp\"\n        />\n        <source\n          srcset=\"/static/16c1db1f2ea6b920668c0c2e3c7eb04f/36ca5/polyglot-python-app-architecture.png 200w,\n/static/16c1db1f2ea6b920668c0c2e3c7eb04f/a3397/polyglot-python-app-architecture.png 400w,\n/static/16c1db1f2ea6b920668c0c2e3c7eb04f/50223/polyglot-python-app-architecture.png 695w\"\n          sizes=\"(max-width: 695px) 100vw, 695px\"\n          type=\"image/png\"\n        />\n        <img\n          class=\"gatsby-resp-image-image\"\n          src=\"/static/16c1db1f2ea6b920668c0c2e3c7eb04f/50223/polyglot-python-app-architecture.png\"\n          alt=\"SCDF Python Tasks\"\n          title=\"SCDF Python Tasks\"\n          loading=\"lazy\"\n          style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        />\n      </picture>\n    </span></p>\n<p>As timestamp source will use the prebuilt <a href=\"https://docs.spring.io/spring-cloud-stream-app-starters/docs/Einstein.SR7/reference/htmlsingle/#spring-cloud-stream-modules-time-source\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Time Source</a> application but registered as Data Flow <code class=\"language-text\">App</code> type.\nIt continuously emits timestamps to a downstream Kafka topic called <code class=\"language-text\">timeDest</code>.</p>\n<p>The <code class=\"language-text\">Router</code> app, implemented by the Python script and packaged as a Docker image, consumes the incoming timestamps from the <code class=\"language-text\">timeDest</code> Kafka topic and according to the timestamp value routes the messages downstream to either the <code class=\"language-text\">evenDest</code> or <code class=\"language-text\">oddDest</code> Kafka topics.</p>\n<p>The <code class=\"language-text\">Even Logger</code> and <code class=\"language-text\">Odd Logger</code> components are the prebuilt <a href=\"https://docs.spring.io/spring-cloud-stream-app-starters/docs/Einstein.SR7/reference/htmlsingle/#spring-cloud-stream-modules-log-sink\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Log Sink</a> applications but registered as Data Flow <code class=\"language-text\">App</code> type.\nLoggers consume the <code class=\"language-text\">evenDest</code> or <code class=\"language-text\">oddDest</code> topics and prints the incoming message in on the console.</p>\n<p>Apache Kafka will be used as the messaging middleware.</p>\n<h2 id=\"development\" style=\"position:relative;\"><a href=\"#development\" aria-label=\"development permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Development</h2>\n<p>The source code can be found in the samples GitHub <a href=\"https://github.com/spring-cloud/spring-cloud-dataflow-samples/tree/master/dataflow-website/recipes/polyglot/polyglot-python-app\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">repository</a> and downloaded as a zipped archive: <a href=\"https://github.com/spring-cloud/spring-cloud-dataflow-samples/raw/master/dataflow-website/recipes/polyglot/polyglot-python-app.zip\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">polyglot-python-app.zip</a>.</p>\n<p>The <a href=\"https://github.com/spring-cloud/spring-cloud-dataflow-samples/blob/master/dataflow-website/recipes/polyglot/polyglot-python-app/python_router_app.py\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">python<em>router</em>app.py</a> implements the timestamp Router application logic.</p>\n<div class=\"spring-code\">\n        <button onClick=\"codeToClipboard(event)\" class=\"button button-clipboard\" type=\"button\">Copy</button>\n        <div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> kafka <span class=\"token keyword\">import</span> KafkaConsumer<span class=\"token punctuation\">,</span> KafkaProducer\n<span class=\"token keyword\">from</span> kafka<span class=\"token punctuation\">.</span>admin <span class=\"token keyword\">import</span> KafkaAdminClient<span class=\"token punctuation\">,</span> NewTopic\n<span class=\"token keyword\">from</span> kafka<span class=\"token punctuation\">.</span>errors <span class=\"token keyword\">import</span> TopicAlreadyExistsError\n\n<span class=\"token keyword\">from</span> util<span class=\"token punctuation\">.</span>actuator <span class=\"token keyword\">import</span> Actuator\n<span class=\"token keyword\">from</span> util<span class=\"token punctuation\">.</span>arguments <span class=\"token keyword\">import</span> get_kafka_brokers<span class=\"token punctuation\">,</span> get_env_info<span class=\"token punctuation\">,</span> get_channel_topic\n\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">Router</span><span class=\"token punctuation\">:</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> info<span class=\"token punctuation\">,</span> kafka_brokers<span class=\"token punctuation\">,</span> input_topic<span class=\"token punctuation\">,</span> even_topic<span class=\"token punctuation\">,</span> odd_topic<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n\n        self<span class=\"token punctuation\">.</span>kafka_brokers <span class=\"token operator\">=</span> kafka_brokers\n        self<span class=\"token punctuation\">.</span>input_topic <span class=\"token operator\">=</span> input_topic\n        self<span class=\"token punctuation\">.</span>even_topic <span class=\"token operator\">=</span> even_topic\n        self<span class=\"token punctuation\">.</span>odd_topic <span class=\"token operator\">=</span> odd_topic\n\n        <span class=\"token comment\"># Serve the liveliness and readiness probes via http server in a separate thread.</span>\n        Actuator<span class=\"token punctuation\">.</span>start<span class=\"token punctuation\">(</span>port<span class=\"token operator\">=</span><span class=\"token number\">8080</span><span class=\"token punctuation\">,</span> info<span class=\"token operator\">=</span>info<span class=\"token punctuation\">)</span>\n\n        <span class=\"token comment\"># Ensure the output topics exist.</span>\n        self<span class=\"token punctuation\">.</span>__create_topics_if_missing<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>self<span class=\"token punctuation\">.</span>input_topic<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">.</span>even_topic<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">.</span>odd_topic<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n        self<span class=\"token punctuation\">.</span>consumer <span class=\"token operator\">=</span> KafkaConsumer<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>input_topic<span class=\"token punctuation\">,</span> bootstrap_servers<span class=\"token operator\">=</span>self<span class=\"token punctuation\">.</span>kafka_brokers<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>producer <span class=\"token operator\">=</span> KafkaProducer<span class=\"token punctuation\">(</span>bootstrap_servers<span class=\"token operator\">=</span>self<span class=\"token punctuation\">.</span>kafka_brokers<span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__create_topics_if_missing</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> topic_names<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        admin_client <span class=\"token operator\">=</span> KafkaAdminClient<span class=\"token punctuation\">(</span>bootstrap_servers<span class=\"token operator\">=</span>self<span class=\"token punctuation\">.</span>kafka_brokers<span class=\"token punctuation\">,</span> client_id<span class=\"token operator\">=</span><span class=\"token string\">'test'</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">for</span> topic <span class=\"token keyword\">in</span> topic_names<span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">try</span><span class=\"token punctuation\">:</span>\n                new_topic <span class=\"token operator\">=</span> NewTopic<span class=\"token punctuation\">(</span>name<span class=\"token operator\">=</span>topic<span class=\"token punctuation\">,</span> num_partitions<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> replication_factor<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n                admin_client<span class=\"token punctuation\">.</span>create_topics<span class=\"token punctuation\">(</span>new_topics<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span>new_topic<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> validate_only<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span>\n            <span class=\"token keyword\">except</span> TopicAlreadyExistsError<span class=\"token punctuation\">:</span>\n                <span class=\"token keyword\">print</span> <span class=\"token punctuation\">(</span><span class=\"token string\">'Topic: {} already exists!'</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">process_timestamps</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">while</span> <span class=\"token boolean\">True</span><span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">for</span> message <span class=\"token keyword\">in</span> self<span class=\"token punctuation\">.</span>consumer<span class=\"token punctuation\">:</span>\n                <span class=\"token keyword\">if</span> message<span class=\"token punctuation\">.</span>value <span class=\"token keyword\">is</span> <span class=\"token keyword\">not</span> <span class=\"token boolean\">None</span><span class=\"token punctuation\">:</span>\n                    <span class=\"token keyword\">if</span> self<span class=\"token punctuation\">.</span>is_even_timestamp<span class=\"token punctuation\">(</span>message<span class=\"token punctuation\">.</span>value<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n                        self<span class=\"token punctuation\">.</span>producer<span class=\"token punctuation\">.</span>send<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>even_topic<span class=\"token punctuation\">,</span> <span class=\"token string\">b'Even timestamp: '</span> <span class=\"token operator\">+</span> message<span class=\"token punctuation\">.</span>value<span class=\"token punctuation\">)</span>\n                    <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n                        self<span class=\"token punctuation\">.</span>producer<span class=\"token punctuation\">.</span>send<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>odd_topic<span class=\"token punctuation\">,</span> <span class=\"token string\">b'Odd timestamp:'</span> <span class=\"token operator\">+</span> message<span class=\"token punctuation\">.</span>value<span class=\"token punctuation\">)</span>\n\n    <span class=\"token decorator annotation punctuation\">@staticmethod</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">is_even_timestamp</span><span class=\"token punctuation\">(</span>value<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> <span class=\"token builtin\">int</span><span class=\"token punctuation\">(</span>value<span class=\"token punctuation\">[</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">%</span> <span class=\"token number\">2</span> <span class=\"token operator\">==</span> <span class=\"token number\">0</span>\n\n\nRouter<span class=\"token punctuation\">(</span>\n    get_env_info<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    get_kafka_brokers<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    get_channel_topic<span class=\"token punctuation\">(</span><span class=\"token string\">'input'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    get_channel_topic<span class=\"token punctuation\">(</span><span class=\"token string\">'even'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    get_channel_topic<span class=\"token punctuation\">(</span><span class=\"token string\">'odd'</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>process_timestamps<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n      </div>\n<div class=\"custom-block admonition note\"><div class=\"custom-block-body\"><p>If the <code class=\"language-text\">print</code> command is used inside the Python script, later must be flushed with <code class=\"language-text\">sys.stdout.flush()</code> to prevent the output buffer being filled up, causing disruption to the Kafkaâ€™s consumer/producer flow!</p></div></div>\n<ul>\n<li>The <a href=\"https://github.com/dpkp/kafka-python\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">kafka-python</a> library is used to consume and produce Kafka messages. The process_timestamps method continuously consumes timestamps from the input channel and routs the even or odd values to the output channels.</li>\n<li>The <a href=\"https://github.com/spring-cloud/spring-cloud-dataflow-samples/blob/master/dataflow-website/recipes/polyglot/polyglot-python-app/util/actuator.py#L7\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Actuator</a> class inside <a href=\"https://github.com/spring-cloud/spring-cloud-dataflow-samples/blob/master/dataflow-website/recipes/polyglot/polyglot-python-app/util/actuator.py\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">actuator.py</a> utility is used to expose operational information about the running application, such as health, liveliness, info, etc.\nIt runs an embedded HTTP server in a separate thread and exposes the <code class=\"language-text\">/actuator/health</code> and <code class=\"language-text\">/actuator/info</code> entry-points handles the Kubernetes liveness and readiness probes requests.</li>\n<li>The <a href=\"https://github.com/spring-cloud/spring-cloud-dataflow-samples/blob/master/dataflow-website/recipes/polyglot/polyglot-python-app/util/arguments.py\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">arguments.py</a> utility helps to retrieve the required input parameters from the command line arguments and environment variables.\nThe utility assumes default (e.g. exec) <a href=\"https://docs.spring.io/spring-cloud-dataflow/docs/2.6.3/reference/htmlsingle/#_entry_point_style_2\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">entry point style</a>.\nNote that Data Flow passes the Kafka broker connection properties as environment variables.</li>\n</ul>\n<p>For the <code class=\"language-text\">python_router_app.py</code> to act as a Data Flow <code class=\"language-text\">app</code> it needs to be bundled in a docker image and uploaded to <code class=\"language-text\">DockerHub</code>. Following <a href=\"https://github.com/spring-cloud/spring-cloud-dataflow-samples/blob/master/dataflow-website/recipes/polyglot/polyglot-python-app/Dockerfile\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Dockerfile</a> illustrates how to bundle a Python script into docker image:</p>\n<div class=\"spring-code\">\n        <button onClick=\"codeToClipboard(event)\" class=\"button button-clipboard\" type=\"button\">Copy</button>\n        <div class=\"gatsby-highlight\" data-language=\"docker\"><pre class=\"language-docker\"><code class=\"language-docker\"><span class=\"token instruction\"><span class=\"token keyword\">FROM</span> python:3.7.3-slim</span>\n<span class=\"token instruction\"><span class=\"token keyword\">RUN</span> pip install kafka-python</span>\n<span class=\"token instruction\"><span class=\"token keyword\">RUN</span> pip install flask</span>\n<span class=\"token instruction\"><span class=\"token keyword\">ADD</span> /util/* /util/</span>\n<span class=\"token instruction\"><span class=\"token keyword\">ADD</span> python_router_app.py /</span>\n<span class=\"token instruction\"><span class=\"token keyword\">ENTRYPOINT</span> [<span class=\"token string\">\"python\"</span>,<span class=\"token string\">\"/python_router_app.py\"</span>]</span>\n<span class=\"token instruction\"><span class=\"token keyword\">CMD</span> []</span></code></pre></div>\n      </div>\n<p>The Dockerfile installs the required dependencies, adds the python script (e.g. <code class=\"language-text\">ADD python_router_app.py</code>) and utilities (under the <code class=\"language-text\">util</code> folder above) and sets the command entry.</p>\n<h3 id=\"build\" style=\"position:relative;\"><a href=\"#build\" aria-label=\"build permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Build</h3>\n<p>We will now build the docker image and push it to the DockerHub registry.</p>\n<p>Checkout the <a href=\"https://github.com/spring-cloud/spring-cloud-dataflow-samples\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">sample project</a> and navigate to the <code class=\"language-text\">polyglot-python-app</code> folder:</p>\n<div class=\"spring-code\">\n        <button onClick=\"codeToClipboard(event)\" class=\"button button-clipboard\" type=\"button\">Copy</button>\n        <div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token function\">git</span> clone https://github.com/spring-cloud/spring-cloud-dataflow-samples\n<span class=\"token builtin class-name\">cd</span> ./spring-cloud-dataflow-samples/dataflow-website/recipes/polyglot/polyglot-python-app/</code></pre></div>\n      </div>\n<p>From within the <code class=\"language-text\">polyglot-python-app</code>, build and push the polyglot-python-app Docker image to DockerHub:</p>\n<div class=\"spring-code\">\n        <button onClick=\"codeToClipboard(event)\" class=\"button button-clipboard\" type=\"button\">Copy</button>\n        <div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">docker build -t springcloud/polyglot-python-app:0.2 <span class=\"token builtin class-name\">.</span>\ndocker push springcloud/polyglot-python-app:0.2</code></pre></div>\n      </div>\n<div class=\"admonition tip\"><p>Replace <code class=\"language-text\">springcloud</code> with your docker hub prefix.</p></div>\n<p>Once published in Docker Hub, the image can be registered in Data Flow and deployed.</p>\n<h2 id=\"deployment\" style=\"position:relative;\"><a href=\"#deployment\" aria-label=\"deployment permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Deployment</h2>\n<p>Follow the <a href=\"/docs/2.6.x/installation/kubernetes/\">installation instructions</a> to set up Data Flow on Kubernetes.</p>\n<p>Retrieve the Data Flow url from minikube (<code class=\"language-text\">minikube service --url scdf-server</code>) and configure your Data Flow shell:</p>\n<div class=\"spring-code\">\n        <button onClick=\"codeToClipboard(event)\" class=\"button button-clipboard\" type=\"button\">Copy</button>\n        <div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">dataflow config server --uri http://192.168.99.100:30868</code></pre></div>\n      </div>\n<p>Import the SCDF <code class=\"language-text\">time</code> and <code class=\"language-text\">log</code> app starters and register the polyglot-python-app as <code class=\"language-text\">python-router</code> of type <code class=\"language-text\">app</code></p>\n<div class=\"spring-code\">\n        <button onClick=\"codeToClipboard(event)\" class=\"button button-clipboard\" type=\"button\">Copy</button>\n        <div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">app register --name <span class=\"token function\">time</span> --type app --uri docker:springcloudstream/time-source-kafka:2.1.0.RELEASE --metadata-uri maven://org.springframework.cloud.stream.app:time-source-kafka:jar:metadata:2.1.0.RELEASE\n\napp register --name log --type app --uri docker:springcloudstream/log-sink-kafka:2.1.1.RELEASE --metadata-uri maven://org.springframework.cloud.stream.app:log-sink-kafka:jar:metadata:2.1.1.RELEASE\n\napp register --type app --name python-router --uri docker://springcloud/polyglot-python-app:0.2</code></pre></div>\n      </div>\n<p>The <code class=\"language-text\">docker://springcloud/polyglot-python-app:0.2</code> is resolved from the <a href=\"https://hub.docker.com/r/springcloud/polyglot-python-app\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">DockerHub repository</a>.</p>\n<p>Create the timestamp routing Stream pipeline:</p>\n<div class=\"spring-code\">\n        <button onClick=\"codeToClipboard(event)\" class=\"button button-clipboard\" type=\"button\">Copy</button>\n        <div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">stream create --name timeStampStream --definition <span class=\"token string\">\"time || python-router || evenLogger: log || oddLogger: log\"</span></code></pre></div>\n      </div>\n<div class=\"admonition note\"><p>The stream definitions above make use of the <a href=\"/docs/2.6.x/feature-guides/streams/labels/\">label feature</a> in the DSL.</p></div>\n<p>As result the following stream pipeline is created:</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 800px;\"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 33%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAIAAACHqfpvAAAACXBIWXMAAAsSAAALEgHS3X78AAAAy0lEQVQY03WQW3LCMAxFs/99sA0WUP66ACYNAyVA4tjW0yYEmUxJh5Yz+tDofVUhESAyCzMTs2pCJFUx31I559t7KlG9/pBzMWN8MDtz3fSLpZmAnlErFdHpPyyFdo2ITXwGq+hxiLFzQ++88yHlshsGEk7pmhEYL5QgBcJ6XR++2kjLsmp/bJHYRyhji4QRlXfr5lz3Qel7e7qsDmHjgmDz0bT7MwovzW7wUH5G+XHPLM+mAKDpLZGx1EnS/rMLXTRnaX7R9vc303vuLkqYvb56KvkAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n        <source\n          srcset=\"/static/b92f469e4459d16304efceae5242939e/507e6/polyglot-python-app-timeStampStream-undeployed.webp 200w,\n/static/b92f469e4459d16304efceae5242939e/28a80/polyglot-python-app-timeStampStream-undeployed.webp 400w,\n/static/b92f469e4459d16304efceae5242939e/8d2ea/polyglot-python-app-timeStampStream-undeployed.webp 800w,\n/static/b92f469e4459d16304efceae5242939e/68fc1/polyglot-python-app-timeStampStream-undeployed.webp 1200w,\n/static/b92f469e4459d16304efceae5242939e/43d96/polyglot-python-app-timeStampStream-undeployed.webp 1600w,\n/static/b92f469e4459d16304efceae5242939e/702fc/polyglot-python-app-timeStampStream-undeployed.webp 2542w\"\n          sizes=\"(max-width: 800px) 100vw, 800px\"\n          type=\"image/webp\"\n        />\n        <source\n          srcset=\"/static/b92f469e4459d16304efceae5242939e/36ca5/polyglot-python-app-timeStampStream-undeployed.png 200w,\n/static/b92f469e4459d16304efceae5242939e/a3397/polyglot-python-app-timeStampStream-undeployed.png 400w,\n/static/b92f469e4459d16304efceae5242939e/a331c/polyglot-python-app-timeStampStream-undeployed.png 800w,\n/static/b92f469e4459d16304efceae5242939e/8537d/polyglot-python-app-timeStampStream-undeployed.png 1200w,\n/static/b92f469e4459d16304efceae5242939e/1a152/polyglot-python-app-timeStampStream-undeployed.png 1600w,\n/static/b92f469e4459d16304efceae5242939e/57a09/polyglot-python-app-timeStampStream-undeployed.png 2542w\"\n          sizes=\"(max-width: 800px) 100vw, 800px\"\n          type=\"image/png\"\n        />\n        <img\n          class=\"gatsby-resp-image-image\"\n          src=\"/static/b92f469e4459d16304efceae5242939e/a331c/polyglot-python-app-timeStampStream-undeployed.png\"\n          alt=\"timeStampStream un-deployed\"\n          title=\"timeStampStream un-deployed\"\n          loading=\"lazy\"\n          style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        />\n      </picture>\n    </span></p>\n<div class=\"admonition important\"><p>The <code class=\"language-text\">time</code>, <code class=\"language-text\">log</code> and <code class=\"language-text\">python-router</code> are registered as <a href=\"https://docs.spring.io/spring-cloud-dataflow/docs/2.6.3/reference/htmlsingle/#spring-cloud-dataflow-stream-app-dsl\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">App</a> type applications and therefore can have multiple input and output bindings (e.g. channels). Data Flow does not make any assumptions about the flow of data from one application to another. It is the developerâ€™s responsibility to 'wire up' the multiple applications when deploying in order for them to communicate.</p></div>\n<p>Keeping this in mind we deploy the timestamp Stream pipeline with the <a href=\"https://github.com/spring-cloud/spring-cloud-dataflow-samples/blob/master/dataflow-website/recipes/polyglot/polyglot-python-app/polyglot-python-app-deployment.properties\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">polyglot-python-app-deployment.properties</a> deployment properties:</p>\n<div class=\"spring-code\">\n        <button onClick=\"codeToClipboard(event)\" class=\"button button-clipboard\" type=\"button\">Copy</button>\n        <div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">stream deploy --name timeStampStream --propertiesFile <span class=\"token operator\">&lt;</span>polyglot-python-app folder<span class=\"token operator\">></span>/polyglot-python-app-deployment.properties</code></pre></div>\n      </div>\n<p>The deployment properties defines the Kafka topics used to wire the time, python-router and logger applications:</p>\n<div class=\"spring-code\">\n        <button onClick=\"codeToClipboard(event)\" class=\"button button-clipboard\" type=\"button\">Copy</button>\n        <div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">app.time.spring.cloud.stream.bindings.output.destination=timeDest\n\napp.python-router.spring.cloud.stream.bindings.input.destination=timeDest\napp.python-router.spring.cloud.stream.bindings.even.destination=evenDest\napp.python-router.spring.cloud.stream.bindings.odd.destination=oddDest\n\napp.evenLogger.spring.cloud.stream.bindings.input.destination=evenDest\napp.oddLogger.spring.cloud.stream.bindings.input.destination=oddDest</code></pre></div>\n      </div>\n<div class=\"custom-block admonition tip\"><div class=\"custom-block-body\"><p>the app.python-router.xxx prefix is a Data Flow convention to map the properties specified after the prefix to the python-router app in the timeStampStream stream.</p></div></div>\n<p>The timestamp channel is bound to the <code class=\"language-text\">timeDest</code> Kafka topic, the router's even output channel is bound to the <code class=\"language-text\">evenDest</code> topic and the odd channel is bound to the <code class=\"language-text\">oddDest</code> topic.\nAfter the deployment the data flow looks like this:</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 800px;\"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 41.5%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsSAAALEgHS3X78AAABPklEQVQoz3VSa1ODMBDk//81dcapbT+0aKFVW0BKLCEhL9a7hDrq2DA7R5J77iabpgl/cWv99AmMkOBDiJbPs/8C2cH6AOeTtS7AOI+LMmh7DTlaaOMw2hBh3ETWx9iMg9hBkQND04UYKPAy4pOsob2JCVNiTsCdcFzT96hEh1PXoR0kPHcY5ktuWVuH17bFG4HtWap4Lg0VkD3OFNSpgRrgvcGiKLHYbnC3WmJ7OsLQZL9G7rTC82GPl32J4viOsu2oSMDmWON+ucQ632KR53g6fGAYXaKHEAjG+8Qhj8HcsENPo9fUVUWoBwWhTRKBPuWJs+AjLPHZ9CMedwUe1qvUYVOB5fwe2c+K8TKWi9gIqWd+uSD9C3nllZJeJE5CEIcCZ6WSKLeex/UppGeRisaEQ5qGxYsqz0LxpLy+AHq2bgZAlnfDAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n        <source\n          srcset=\"/static/45d995420fbcf32967e1d5f793a85c06/507e6/polyglot-python-app-timeStampStream-deployed.webp 200w,\n/static/45d995420fbcf32967e1d5f793a85c06/28a80/polyglot-python-app-timeStampStream-deployed.webp 400w,\n/static/45d995420fbcf32967e1d5f793a85c06/8d2ea/polyglot-python-app-timeStampStream-deployed.webp 800w,\n/static/45d995420fbcf32967e1d5f793a85c06/fd28f/polyglot-python-app-timeStampStream-deployed.webp 1167w\"\n          sizes=\"(max-width: 800px) 100vw, 800px\"\n          type=\"image/webp\"\n        />\n        <source\n          srcset=\"/static/45d995420fbcf32967e1d5f793a85c06/36ca5/polyglot-python-app-timeStampStream-deployed.png 200w,\n/static/45d995420fbcf32967e1d5f793a85c06/a3397/polyglot-python-app-timeStampStream-deployed.png 400w,\n/static/45d995420fbcf32967e1d5f793a85c06/a331c/polyglot-python-app-timeStampStream-deployed.png 800w,\n/static/45d995420fbcf32967e1d5f793a85c06/1f0fd/polyglot-python-app-timeStampStream-deployed.png 1167w\"\n          sizes=\"(max-width: 800px) 100vw, 800px\"\n          type=\"image/png\"\n        />\n        <img\n          class=\"gatsby-resp-image-image\"\n          src=\"/static/45d995420fbcf32967e1d5f793a85c06/a331c/polyglot-python-app-timeStampStream-deployed.png\"\n          alt=\"timeStampStream deployed\"\n          title=\"timeStampStream deployed\"\n          loading=\"lazy\"\n          style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        />\n      </picture>\n    </span></p>\n<ul>\n<li>\n<p>Use <code class=\"language-text\">kubectl get all</code> command to list the statuses of the deployed k8s containers. Use <code class=\"language-text\">kubectl logs -f xxx</code> to observe the even and odd pipeline output.</p>\n<p>For example the <code class=\"language-text\">kubectl logs -f po/timestampstream-evenlogger-xxx</code> should output:</p>\n<div class=\"spring-code\">\n        <button onClick=\"codeToClipboard(event)\" class=\"button button-clipboard\" type=\"button\">Copy</button>\n        <div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token number\">2019</span>-05-17 <span class=\"token number\">17</span>:56:36.241  INFO <span class=\"token number\">1</span> --- log-sink   <span class=\"token builtin class-name\">:</span> Even timestamp:05/17/19 <span class=\"token number\">17</span>:56:36\n<span class=\"token number\">2019</span>-05-17 <span class=\"token number\">17</span>:56:38.301  INFO <span class=\"token number\">1</span> --- log-sink   <span class=\"token builtin class-name\">:</span> Even timestamp:05/17/19 <span class=\"token number\">17</span>:56:38\n<span class=\"token number\">2019</span>-05-17 <span class=\"token number\">17</span>:56:40.402  INFO <span class=\"token number\">1</span> --- log-sink   <span class=\"token builtin class-name\">:</span> Even timestamp:05/17/19 <span class=\"token number\">17</span>:56:40\n<span class=\"token punctuation\">..</span>.</code></pre></div>\n      </div>\n<p>and the <code class=\"language-text\">kubectl logs -f po/timestampstream-oddlogger-xxx</code> should output:</p>\n<div class=\"spring-code\">\n        <button onClick=\"codeToClipboard(event)\" class=\"button button-clipboard\" type=\"button\">Copy</button>\n        <div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token number\">2019</span>-05-17 <span class=\"token number\">17</span>:56:37.447  INFO <span class=\"token number\">1</span> --- log-sink   <span class=\"token builtin class-name\">:</span> Odd timestamp:05/17/19 <span class=\"token number\">17</span>:56:37\n<span class=\"token number\">2019</span>-05-17 <span class=\"token number\">17</span>:56:39.358  INFO <span class=\"token number\">1</span> --- log-sink   <span class=\"token builtin class-name\">:</span> Odd timestamp:05/17/19 <span class=\"token number\">17</span>:56:39\n<span class=\"token number\">2019</span>-05-17 <span class=\"token number\">17</span>:56:41.452  INFO <span class=\"token number\">1</span> --- log-sink   <span class=\"token builtin class-name\">:</span> Odd timestamp:05/17/19 <span class=\"token number\">17</span>:56:41\n<span class=\"token punctuation\">..</span>.</code></pre></div>\n      </div>\n</li>\n</ul>","headings":[{"value":"Create and Deploy a Python Application","depth":1},{"value":"Development","depth":2},{"value":"Build","depth":3},{"value":"Deployment","depth":2}],"fields":{"path":"/docs/2.6.x/recipes/polyglot/app/","version":"2.6.x","category":"recipes","sourcePath":"pages/6-recipes/1-polyglot/3-polyglot-python-app.md"},"frontmatter":{"title":"Python Application","summary":null,"path":"recipes/polyglot/app/","toc":null,"prevNext":null}}},"pageContext":{"slug":"/docs/2.6.x/recipes/polyglot/app/","version":"2.6.x","versionPath":""}},"staticQueryHashes":["2044043181"]}